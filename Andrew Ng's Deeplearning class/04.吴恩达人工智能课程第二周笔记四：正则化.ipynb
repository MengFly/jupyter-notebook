{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  吴恩达人工智能课程第二周笔记四：正则化\n",
    "当你怀疑你的模型过度拟合了数据，即存在高方差的情况，通常情况下你可以通过正则化和提供更过数据来解决这个问题。\n",
    "\n",
    "## 正则化作用原理：\n",
    "\n",
    "### L2 正则化\n",
    "在logistc回归中，我们要求损失函数的最小值，损失函数如下： \n",
    "\n",
    "$$ J(w, b) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{(i)}, y^{(i)}) $$\n",
    "\n",
    "其中，w是一个多维参数，b是一个实数 \n",
    "在回归函数中添加**正则化**，公式就变成如下：  \n",
    "\n",
    "$$ J(w, b) = \\frac{1}{m}\\sum_{i = 1}^{m}{L(\\hat{y}^{(i)}, y^{(i)})} + \\frac{\\lambda}{2m}||w||_{2}^{2} $$\n",
    "\n",
    "这里 \\\\(\\left \\| w \\right \\|_2^2\\\\)的公式为：  \n",
    "\n",
    "$$||w||_{2}^{2} = \\sum_{j = 1}^{n_x}{w_j^2} = w^Tw $$\n",
    "\n",
    "这个方法也成为**L2正则化**，因为这里用了欧几里得法线，被称为**参数W的L2范数**\n",
    "\n",
    "为什么只正则化w参数呢？这里不是还有一个参数b吗？为什么不加上\\\\(\\frac{\\lambda}{m}||b||_2^2\\\\) ?\n",
    "其实也可以这么做，只是可以省略不写，因为w通常是一个高维参数矢量，已经可以表达高偏差问题。而b只是一个实数，因此w几乎涵盖所有的参数，而b不是。因此加了b也不会有什么太大的影响。\n",
    "\n",
    "### L1 正则化\n",
    "L1正则化：如果在损失函数后面添加的不是L2范数，而是L1范数即：\n",
    "\n",
    "$$ \\frac{\\lambda}{m}\\sum_{j = 1}^{n_x}{w_j} = \\frac{\\lambda}{m}||w||_1 $$\n",
    "\n",
    "无论分母是m还是2m，它只是一个比例常量。\n",
    "\n",
    "如果你用了L1模型，w最终会是稀疏的，也就是说w向量中有很多0，事实上L1正则化使w稀疏化，并没有降低太多的存储内存。因此L1正则化并不是为了压缩模型。\n",
    "\n",
    "人们在训练模型的时候，越来越倾向于使用L2正则化。\n",
    "\n",
    "λ是一个正则化参数，我们通常使用验证集来配置这个参数，尝试各种各样的数据。寻找更好的参数，我们要考虑训练集之间的权衡，把参数正常值设为较小值，这样可以避免过拟合。λ是一个需要调整的超级参数\n",
    "因为上面例子谈论的是在logistic回归中的正则化情况，因此其中的w参数是一个\\\\([n_x, 1]\\\\)的向量，那么上面的W的L2范式就好理解了，如下。\n",
    "\n",
    "$$||w||_{2}^{2} = \\sum_{j = 1}^{n_x}{w_j^2} = w^Tw = [w_1, w_2,...,w_{nx}] \\cdot \\begin{bmatrix}\n",
    "w_1 \\\\ \n",
    "w_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "w_{nx}\\\\\n",
    "\\end{bmatrix}  = w_1^2 + w_2^2 + ... + w_{nx}^2 $$\n",
    "\n",
    "### 在经网络里面怎么运用L2正则化？\n",
    "\n",
    "在神经网络中我们有一个损失函数：\n",
    "\n",
    "$$ J(w^{[1]}, b^{[1]}, w^{[2]}, b^{[2]},...,w^{[L], b^{[L]}}) = \\frac{1}{m}\\sum_{i = 1}^{m}(\\hat{y}^{(i)}, y^{(i)}) $$\n",
    "\n",
    "正则项为：\n",
    "\n",
    "$$ \\frac{1}{m} \\sum_{l = 1}^{L}||w^{[l]}||^2 $$\n",
    "\n",
    "这个矩阵范数被定义为矩阵中所有元素的平方求和。即：\n",
    "\n",
    "$$ ||w^{[l]}||^2 = \\sum_{i = 1}^{n^{[l-1]}}{\\sum_{j = 1}^{n^{[l]}}{(w_{ij}^{[l]})^2}} $$\n",
    "\n",
    "因为对于第l层的参数w来说，它的维度是 \\\\( (n^{[l-1]}, n^{[l]}) \\\\)\n",
    "该矩阵范数被称作“弗罗贝尼乌斯范数”\n",
    "\n",
    "如何使用该范数计算梯度下降呢？\n",
    "我们之前的梯度下降法公式如下：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    " & dw^{[l]} = \\frac{\\partial J}{\\partial w^{[l]}} \\\\  \n",
    " & w^{[l]} = w^{[l]} - \\alpha dw^{[1]}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "添加正则项之后，公式应该变为如下：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    " & dw^{[l]} = \\frac{\\partial J}{\\partial w^{[l]}} + \\frac{\\lambda}{m}{w^{[l]}} \\\\  \n",
    " & w^{[l]} = w^{[l]} - \\alpha dw^{[1]}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "这也是L2正则化被称为权重衰减的原因，因为最终的w的更新函数将变成下面这样：  \n",
    "\n",
    "$$ w^{[l]} = (1 - \\alpha \\frac{\\lambda}{m}){w^{[l]}} - \\alpha \\frac{\\partial J}{\\partial w^{[l]}}$$\n",
    "\n",
    "该正则项说明，不论\\\\( w^{[l]} \\\\)是什么，我们都试图让它变得更小，因此L2范数正则化也被成为权重衰减"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
