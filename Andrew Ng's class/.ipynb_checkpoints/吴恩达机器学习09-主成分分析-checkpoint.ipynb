{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    "降维的主要目的就是减少冗余。\n",
    "\n",
    "降维给我们提供了一种可视化数据的途径，通过对数据的降维可以达到可视化的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 主成分分析法\n",
    "\n",
    "PCA会寻找投影误差最小的投影平面进行投影，所谓投影误差是指投影的点到投影平面的距离。\n",
    "\n",
    "因此如果要应用PCA算法最好把各个特征标准化，这样就能更好的使用算法衡量投影结果。\n",
    "\n",
    "#### PCA算法步骤\n",
    "\n",
    "Principal Component Analysis （PCA） algorithm\n",
    "\n",
    "Reduce data from n-dimensions to k-dimensions\n",
    "\n",
    "1. Compute “covariance matrix”\n",
    "$$\\Sigma = \\frac{1}{m}\\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T$$\n",
    "In this method, Finally $\\Sigma \\in \\mathbb{R^n}$\n",
    "\n",
    "2. Compute \"eigenvectors\" of matrix $\\Sigma$:\n",
    "Octive: $[U, S, V] = svd(\\Sigma)$\n",
    "\n",
    "3. From [U, S, V], We get:\n",
    "$$U = \\begin{bmatrix} |      & |      &      &    |   \\\\ \n",
    "                     u^{(1)}& u^{(2)}&\\cdots& u^{(n)}\\\\ \n",
    "                     |      & |      &      &    |   \\end{bmatrix}\\in \\mathbb{R}^{n\\times n}$$\n",
    "                     \n",
    "4. From U We get: $U_K = [u^{(1)}, u^{(2)}, \\cdots, u^{(k)}] \\in \\mathbb{R}^{(n \\times k)}$\n",
    "\n",
    "5. And then, we have $z = (U_K)^Tx \\in \\mathbb{R^{(k \\times 1)}}$\n",
    "\n",
    "#### Octave 代码\n",
    "\n",
    "```octave\n",
    "Sigma = 1/m*sum(x(i)*x(i)');\n",
    "\n",
    "[U, S, V] = svd(Sigma);\n",
    "\n",
    "Ureduce = U(:, 1:k);\n",
    "\n",
    "z = Ureduce' * x\n",
    "```\n",
    "\n",
    "#### 怎么选择PCA降维维度？\n",
    "\n",
    "这里有一个度量维度即\n",
    "\n",
    "$$\\frac{\\frac{1}{m}\\sum_{i=1}^{m}\\|x^{(i)}-x_{approx}^{(i)}\\|^2}{\\frac{1}{m}\\sum_{i=1}^{m}\\|x^{(i)}\\|^2} \\le 0.01$$\n",
    "\n",
    "使用上面的奇异值分解中就是：\n",
    "因为 S 是一个对角矩阵，除了对角线上面的值不为零外其他值都为零，并且 $S \\in \\mathbb{R}^{n \\times n}$, 因此，记S对角线上的值为 $s_1, s_2, ..., s_n$。\n",
    "\n",
    "上面的度量标准就相当于：\n",
    "$$1- \\frac{\\sum_{i=1}^ks_i}{\\sum_{i=1}^ns_i} \\le 0.01$$\n",
    "\n",
    "其实上面的标准的意思就是，降维后的数据要依旧能代表元数据，也就是说他们的偏差不能超过 1%.\n",
    "\n",
    "也就是说要保证特征之间99%的差异性。\n",
    "\n",
    "\n",
    "#### 怎么将数据还原\n",
    "\n",
    "首先，从 $x\\rightarrow z$ 的转换过程是 $z = (U_K)^Tx$, 因此就有，再变回 x 的公式如下： $$x = z·U_K$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
