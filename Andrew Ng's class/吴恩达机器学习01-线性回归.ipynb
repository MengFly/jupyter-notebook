{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是机器学习？\n",
    "根据Mitchell在1997年给的定义：“对于某个任务T， 和性能度量P， 一个计算机程序被认为可以从经验 E 中学习指的是：通过经验 E 改进后，它能够在任务 T 上由性能度量 P衡量的性能有所提升。”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习分类\n",
    "- 监督学习（教机器怎么去学习）\n",
    "- 非监督学习（让机器自己去学习）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "我们期望的线形回归函数表达式为 $h_\\theta(x) = \\theta_0 + \\theta_1x$ 而我们期望优化的公式为 $$minimize_{\\theta_0, \\theta_1}\\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "也即有一个损失函数CostFounction, $J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2$ \n",
    "\n",
    "我们的目标就是要让这个CostFunction最小化 $minimize_{\\theta_0, \\theta_1}\\frac{1}{m}J(\\theta_0, \\theta_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降\n",
    "应用梯度下降，重复迭代，不断优化损失函数，让随时函数最小化。\n",
    "\n",
    "这里迭代过程中的优化公式为$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j}J(\\theta_0, \\theta_1)$$\n",
    "\n",
    "另外，应用梯度下降，值得初始化是非常重要的，因为有可能会优化到一个局部最优而非全局最优，当然，某些情况下，局部最优也能保证一定程度的算法准确率。\n",
    "\n",
    "对于线性回归问题来说，它是不存在局部最优问题的，它只有一个全局最优点。\n",
    "\n",
    "另外，学习率的选择宁愿小不可大，因为较大的学习率的步进幅度太大，一是可能很难优化到最优点，二是甚至有可能造成函数的发散无法收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性代数\n",
    "Vector: A vector is an nx1 matrix. such as： $y=\\begin{bmatrix}11 \\\\ 22 \\\\ 33 \\\\ 44\\end{bmatrix}$\n",
    "\n",
    "在Octave中，对矩阵求逆的函数是 *inv(A)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
