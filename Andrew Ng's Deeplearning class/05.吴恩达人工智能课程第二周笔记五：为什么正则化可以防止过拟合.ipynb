{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 吴恩达人工智能课程第二周笔记五：为什么正则化可以防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么正则化有利于预防过拟合呢？为什么它可以减少方差问题？让我们通过两个例子直观体会一下。\n",
    "来看下面的一个过拟合的例子：  \n",
    "<img src=\"img/nerul_network_5_0.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个神经网络添加正则项之后的损失函数为：\n",
    "$$J(w^{[1]}, b^{[1]}, ... , w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}\\left\\|w^{[l]}\\right\\|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**它可以避免数据权值矩阵过大。**\n",
    "\n",
    "直观上理解，就是，如果正则化λ设置得足够大，权重矩阵W被设置为接近0的值，即W≈0，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的影响，如下图。\n",
    "<img src=\"img/nerul_network_5_1.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个被大大简化的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会让神经网络从过拟合状态更接近于下图中中的高偏差状态。\n",
    "<img src=\"img/nerul_network_5_2.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是λ会存在一个中间值，于是会有一个接近just right的中间状态。\n",
    "<img src=\"img/nerul_network_5_3.png\" width=200/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W≈0，实际上不会发生这种情况，我们尝试消除或至少减少许多神经元的影响，最终这个网络会变得简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上该神经网络所有隐藏单元依然存在，但是他们的影响变得更小了，神经网络变得简单了，貌似这样更不容易发生过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个最直观的理解就是，减少神经元中数据权重，可以让深层网络的影响减少，也就是说数据不会拟合的太过于准确，最极致的情况就是深层神经网络像logistic回归那样一样，因此总存在一个影响的中间值，使得权重削减适中，也就是即减少了过拟合的发生。也不至于偏差过大。这是我们直观的想象（至于为什么λ设置很大，深层神经网络会像logistic回归那样我也不是很理解），但是对于正则化的思想就是削弱权重的影响这一点我们可以直观的感觉到。但感觉不同于现实，吴恩达老师说现实的结论中正则化确实会看到方差减少的结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来直观感受一下正则化为什么可以防止过拟合，假设我们用的激活函数是tanh函数.即\\\\(g(z)=tanh(z)\\\\),那么我们发现，只要z非常小，z可以看做是线性的，但是如果z的范围变大，可以看大函数就会变得非线性，如果正则化参数变大，那么激活函数的参数会相对小，因为代价函数中的参数变大了。如果W很小，而\\\\(Z^{[L]} = W^{[L]}A^{[L-1]} + b^{[L]}\\\\), 那么Z也会很小，也就是说g(z)会大致呈线性，每层几乎都是线性的，之前说过，如果深层神经网络每一层都是线性的，那么整个神经网络就是一个线性网络，因此这个网络就不适合做非线性的决策边界，就像上面看到的过度拟合的情况。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意事项**  \n",
    "在应用L2正则化后，我们的损失函数是这样的：\n",
    "$$J(w^{[1]}, b^{[1]}, ... , w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_{l=1}^{L}\\left\\|w^{[l]}\\right\\|^2$$\n",
    "目的是为了预防权重过大，如果你使用的是梯度下降函数，在调试梯度下降时，我们的代价函数应该使用上面的公式，因为J有了一个全新的定义（只是在你使用L2正则化的时候）如果你使用的是之前的代价函数：                                              \n",
    "$$J(w^{[1]}, b^{[1]}, ... , w^{[L]}, b^{[L]}) = \\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}, y^{(i)})$$\n",
    "可能就会看不到单调递减现象。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
