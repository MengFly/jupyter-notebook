{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多参数情况\n",
    "\n",
    "预测公式：$h_\\theta(x) = \\theta^Tx = \\theta_0x_0 + \\theta_1x_1 +  \\theta_2x_2 \\cdots + \\theta_nx_n$\n",
    "\n",
    "参数：$\\theta_0, \\theta_1, \\cdots, \\theta_n$\n",
    "\n",
    "损失函数：$J(\\theta_0, \\theta_1, \\cdots, \\theta_n) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "这里我们知道 $x \\in \\mathbb R^{n+1}, \\theta \\in \\mathbb R^{n+1}$\n",
    "\n",
    "梯度下降： $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\cdots, \\theta_n)$\n",
    " \n",
    "根据求导公式有：由于有 $x_0^{(i)} = 1$ 所以：$$\\theta_i = \\begin{cases} \\theta_i - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) & i = 0 \\\\  \\theta_i - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_{i} & i > 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征缩放（归一化）\n",
    "例如将特征区间缩放到 $[-1, 1]$。\n",
    "\n",
    "一种常用的方式是 $x_i = \\frac{x_i = \\mu}{s_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 判断损失函数是否收敛\n",
    "\n",
    "某些自动收敛测试的原理是如果在一次迭代中损失函数的变化值小于 $10^{-3}$ 就可以认为算法收敛。\n",
    "\n",
    "当然，通过损失函数图像进行观察的方式可能更为直观和方便。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合特征\n",
    "在考虑特征的时候，并非有什么特征就要用什么特征，这里有两点需要注意的\n",
    "1. 挑选合适的特征，有些特征可能和结果没有联系，即它是无用的特征\n",
    "2. 特征组合，某些特征进行运算后会得到新的指标，这个新的指标可能要比原特征更有用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多项式回归\n",
    "其实不单单是多项式回归，我们在处理问题的时候，有时候的数据图像明显不是呈直线趋势，这个时候我们应该根据自己的经验，选择不同的函数进行拟合，多项式，三角函数，平方根函数等等。使用这些函数进行拟合会比之前的线性回归的效果要好得多，当然，前提是，我们需要多这些函数的图像有一个简单的了解才能有的放矢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\theta$ 的直接求解 \n",
    "\n",
    "根据线性代数的知识，我们可以不需要借用梯度下降就可以获取到损失函数最优点的 $\\theta$ 值即 $$\\theta = (x^Tx)^{-1}x^Ty$$\n",
    "\n",
    "在Ovtave中，使用如下代码进行求解 \n",
    "```Octave\n",
    "pinv(x'*x)*x'*y\n",
    "```\n",
    "\n",
    "通过直接的矩阵运算即可得到最优值。我们无需进行迭代，也省去了尝试学习率$\\alpha$, 方便又快捷。但是它也有缺点。\n",
    "1. 对于特征比较多的，比如上万，上百万的参数，$(x^Tx)^{-1}$ 计算代价还是比较大的。\n",
    "2. 通过公式来看，我感觉这个公式智能适用于线性函数参数的求解，至于其他的非线性函数来说并不适用。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
